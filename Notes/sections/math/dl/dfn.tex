\subsubsection{Deep Feedforward Networks}

\begin{remark} \hlt{Deep Feedforward Networks (DFNs)}\\
Defines a mapping $\bm{y} = f(\bm{x}; \bm{\theta})$ and learns value of parameters $\bm{\theta}$ that results in best function approximation.
\end{remark}

\begin{remark} \hlt{Linear Models}\\
Linear models may fit data efficiently and reliably, either in closed form or with convex optimisation.\\
Model capacity limited to linear functions, does not model interaction between any two input variables
\end{remark}

\begin{remark} \hlt{Nonlinear Models}\\
To represent nonlinear functions of $\bm{x}$, apply linear model to transformed input $\phi(\bm{x})$ where $\phi$ is a nonlinear transformation. Kernel trick may be applied to obtain nonlinear learning algorithm. To choose mapping $\phi$:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Choose generic $\phi$, such as that used by kernel machines based on RBF kernel.\\
If $\phi(\bm{x})$ is of high enough dimension, can find enough capacity to fit training set, but generalisations to test set remains poor. Mappings are based on principle of local smoothness and do not encode enough prior information to solve advanced problems.
\item Manually engineer $\phi$. Requires decades of human effort for each task, and practitioners specialising in different domains (i.e, speech recognition, computer vision) with little transfer between domains.
\item Learn $\phi$ through deep learning. Model is $y = f(\bm{x}; \bm{\theta}, \bm{w}) = \phi(\bm{x}; \bm{\theta})^T \bm{w}$, where parameters $\bm{\theta}$ can be used to learn $\phi$ from broad class of functions, and parameters $\bm{w}$ that map from $\phi(\bm{x})$ to desired output.\\
Do not require training problem to be convex, and only require finding the right general function.
\end{enumerate}
\end{remark}

\begin{definition} \hlt{Cost Functions}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Learning Conditional Distributions with Maximum Likelihood: cost function is negative log-likelihood, which is the cross-entropy between training data and model distribution:
\begin{equation}
J(\bm{\theta}) = - \mathbb{E}_{\bm{x}, \bm{y} \sim \hat{p}_{\text{data}}} \log p_{\text{model}}(\bm{y} \ \vert \ \bm{x}) \nonumber
\end{equation}
Specific form of cost function changes from model to model, depending on form of $\log p_{\text{model}}$.\\
Method removes the burden of designing cost functions for each model, as specifying a model $p(\bm{y} \ \vert \ \bm{x})$ automatically determines a cost function $\log p(\bm{y} \ \vert \ \bm{x})$.
\item Learning Conditional Statistics: to learn just one conditional statistic of $\bm{y}$ given $\bm{x}$.\\
With sufficiently powerful neural network, this can represent any function $f$ from a wide class of function, limited by features of continuity and boundedness. Hence the cost function is a functional. Learning is choosing a functional rather than a set of parameters.\\
By calculus of variable, solving the optimisation problem yields the below function,
\begin{align}
f^* &= \arg \min_f \mathbb{E}_{\bm{x}, \bm{y} \sim p_{\text{data}}} \Vert \bm{y} - f(\bm{x}) \Vert^2 \nonumber \\
f^* &= \mathbb{E}_{\bm{y} \sim p_{\text{data}}(\bm{y} \ \vert \ \bm{x})} [\bm{y}] \nonumber
\end{align}
If infinitely many samples from the true data-generating distribution could be trained, then minimising the mean squared error cost function gives a function that predicts mean of $\bm{y}$ for each value of $\bm{x}$.\\
A second result from calculus of variations is:
\begin{align}
f^* = \arg \min_f \mathbb{E}_{\bm{x}, \bm{y} \sim p_{\text{data}}} \Vert \bm{y} - f(\bm{x}) \Vert_1 \nonumber
\end{align}
which is a function that predicts the median value of $\bm{y}$ for each $\bm{x}$. This is the mean absolute error.\\
Note that mean squared error and mean absolute error often lead to poor results when used with gradient-based optimisation. Output units that saturate may produce very small gradients when combined with these cost functions. Hence the reason that cross-entropy cost function is more popular.
\end{enumerate}
\end{definition}



